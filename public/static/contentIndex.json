{"DynamoDB-Stream---Lambda-error-handling":{"title":"Dynamo DB Stream - Lambda error handling","links":["DynamoDB-Stream---Lambda-error-handling"],"tags":[],"content":"Event source configuration\n\n\nFor DynamoDB (as well as SQS) the default batching window is 0 seconds\n\nFor Kinesis, DynamoDB, and Amazon SQS event sources: The default batching window is 0 seconds. This means that Lambda sends batches to your function only when either the batch size is met or the payload size limit is reached.\n\n\nIf you configure a batching window, the next window begins as soon as the previous function invocation completes.\n\n\n\nFor Kinesis and DynamoDB streams, an event source mapping creates an iterator for each shard in the stream and processes items in each shard in order\n\n\nIf an error is returned, the whole batch is reprocessed until the function succeed or the items in the batch expire. To ensure in-order processing, the event source mapping pauses processing for the affected shard until the error is resolved\n\n\nFailure behaviour\nIn case of error, the whole batch will be reprocessed. When would it stop retrying?\n\nA successful exit (lambda completes execution without errors)\nConfigure in the DynamoEventSource a maximum of retry attempts\nConfigure in the DynamoEventSource the max record age. This, by default, is 24 hours for DynamoDB streams\n\nThe default behaviour could be dangerous, as retryAttempts’s default value is to retry until the record expires, which would be 24 hours. If this happens, the following items to be processed in the shard would be stuck until the expiration of the record.\nThat’s why it’s critical to configure reporting batch item failures and even bisecting the batch.\nBatch item failure\nSet reportBatchItemFailures as true in the DynamoEventSource configuration. The identifiers of those failed records should be returned. If unfamiliar with this, give a read to the following article.\nFor this to succeed, the ids of the failed records need to be returned in the lambda response. This can be self-managed or delegated to Lambda Powertools Batch.\nExample\nThree records, the second fails. The identifier of that record will be returned. The stream will identify the lowest sequence number out of those item identifiers received (only one in this case) and retry from there, meaning that the following batch would contain the records 2 and 3.  Retries are counted for the same batch being re-sent.\n\nWith the following configuration:\ntestFunction.addEventSource(new DynamoEventSource(table, {\n  batchSize: 3,\n  retryAttempts: 1,\n  startingPosition: StartingPosition.TRIM_HORIZON,\n  maxBatchingWindow: Duration.seconds(5),\n  filters: [\n    FilterCriteria.filter({\n      eventName: FilterRule.isEqual(&#039;INSERT&#039;),\n    }),\n  ],\n  reportBatchItemFailures: true,\n}));\nAnd inserting three items forcing the second one to fail, the behaviour explained in the image can be seen:\n\nAt that stage, since the retries would have been exhausted (1), the records would not be processed again. If a DLQ is configured, a message about this failed batch would be sent. More information about this message in the section On failure DLQ.\nThe shard would continue with following records.\n\nBisect batch on function error\nIt’s possible to add this configuration in the event source mapping:\ntestFunction.addEventSource(new DynamoEventSource(table, {\n  ...\n  bisectBatchOnError: true,\n}));\nExtracted from AWS Compute Blog by James Beswick:\n\nreportBatchItemFailures + bisectBatchOnError\nBut how would it behave if combining both settings? Quote from bisect on batch error - AWS docs:\n\nIf your invocation fails and BisectBatchOnFunctionError is turned on, the batch is bisected regardless of your ReportBatchItemFailures setting.\nWhen a partial batch success response is received and both BisectBatchOnFunctionError and ReportBatchItemFailures are turned on, the batch is bisected at the returned sequence number and Lambda retries only the remaining records.\n\nThe following image represents the batches that are sent to the lambda given a failure in one of the records.\n\nThe main difference would be that only the record that has consistently failed in all attempts (record 2) would be sent to the DLQ - if configured. If we compare it with the behaviour in section batch item failure, it’s possible to see that in that case the DLQ would have received two records, since no bisecting was taking place.\nOn failure DLQ\nIn the image it’s possible to see an example of what is received in the DLQ when a batch with two records is redirected there. This could happen when not having active the bisectBatchOnErrorconfiguration.\nWhat is received in the DLQ:\n\nNote that if only receiving one failed record in the DLQ message the differences would be:\n\nbatchSize: 1\nstartSequenceNumber and endSequenceNumber hold the same value\n\nThis message contains no data that could give a clue of what went wrong. To get some information, it’s necessary to obtain it from the stream. If this is not done before the record expires (in the stream), then the data will be lost - 24h is the default configuration for a DynamoDB stream.\nThis could be achieved by hydrating the event data, as explained by Yan Cui.\nRetry attempts\nExtracted from the documentation: default configuration is to retry until the event expires (-1) and if given a value, it has to be between 0 and 10k.\nUnexpected behaviour could happen if bisectBatchOnError is configured, since it would split the failed batch and retry, even if setting retryAttempts  to zero. An example of this can be found here.\nRemember that retries are only counted when the same batch is sent again.\n\nRetrying with smaller batches isolates bad records and works around timeout issues. Splitting a batch does not count towards the retry quota.\n\n\nAll the examples are based on processing all records in a batch at least once, even if some failed.\nThis could be handled differently if trying to minimize the number of times a successful message is reprocessed or if the order of execution is relevant. If using Lambda Powertools Batch, the processor BatchProcessorSync could be used.\nReferences\n\nReport batch item failures - AWS Compute Blog - James Beswick \nEvent source mapping batching behaviour\nLambda powertools batch item failures \nServerless Guru lambda integration retries table\nBisect on batch error - AWS docs \nStream partial failures, complete explanation - Marcin Sodkiewicz\nRetry attempts \nDynamo Streams records retention period\nHydrate failed record messages onFailure - Yan Cui \nHow do I troubleshoot DynamoDB Streams in my Lambda functions? \n"},"SQS---Lambda-error-handling":{"title":"SQS - Lambda error hanlding","links":[],"tags":[],"content":"Retries\nLambda retries don’t have any impact. If an error is thrown, the batch will be sent back to the queue. Then, depending on the visibility timeout configured in the queue, the messages of the batch will be reprocessed.\nWhat controls the number of retries in this case is the configuration of a DLQ in the queue that sources the lambda. The field maxReceiveCount will determine how many times a message will be sent to the lambda. Once that number is reached, the message is sent to the DLQ.\nCDK config example:\nconst deadLetterQueue = new Queue(this, &#039;DeadLetterQueue&#039;);\nconst invocationQueue = new Queue(this, &#039;InvocationQueue&#039;, {\n  deadLetterQueue: {\n    queue: deadLetterQueue,\n    maxReceiveCount: 3,\n  }\n});\n \nconst testFunction = new NodejsFunction(...);\ntestFunction.addEventSource(new SqsEventSource(invocationQueue, {\n  batchSize: 10,\n  maxBatchingWindow: Duration.seconds(5),// careful with SQS visibilityTimeout\n}));\nThe message in the DLQ will use the same messageIdand it will contain the message body.\nIt would be necessary to review logs, since it doesn’t show the response/error thrown.\nBatch item failure\nThis configuration will allow to reprocess only those records that failed. To achieve this, configure the event source with reportBatchItemFailures.\nconst testFunction = new NodejsFunction(...);\ntestFunction.addEventSource(new SqsEventSource(invocationQueue, {\n  reportBatchItemFailures: true,\n  batchSize: 10,\n  maxBatchingWindow: Duration.seconds(5),// careful with SQS visibilityTimeout\n}));\nFor this to succeed, the ids of the failed records need to be returned in the lambda response. This can be self-managed or delegated to Lambda Powertools Batch. For a batch of five messages where two failed, the response should be something like this:\n{\n  &quot;batchItemFailures&quot;: [\n    {\n      &quot;itemIdentifier&quot;: &quot;2nd-item-identifier-uuid&quot;\n    },\n    {\n      &quot;itemIdentifier&quot;: &quot;3rd-item-identifier-uuid&quot;\n    }\n  ]\n}\n\n\nA curiosity about Lambda Powertools Batch is that an error is thrown only in the situation of all messages failing. Otherwise, if only one of two messages fail, the error would be “silent” unless logged.\n\n\nAll the examples are based on processing all records in a batch at least once, even if some failed.\nIf using Lambda Powertools Batch, the processor BatchProcessorSync could be used to process the records sequentially. If self-managed, then loop through each record instead of using Promise.all().\nResources\n\nDetailed explanation of SQS Lambda Batch Item Failure behaviour\nLambda powertools batch item failures \nServerless Guru lambda integration retries table\n"},"index":{"title":"index","links":["SQS---Lambda-error-handling","DynamoDB-Stream---Lambda-error-handling"],"tags":[],"content":"SQS - Lambda error handling\nDynamoDB Stream - Lambda error handling"},"to-write-about":{"title":"to-write-about","links":[],"tags":[],"content":"\nHow do DynamoDB stream shards work?\n\ndocs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html#Streams.Processing\naws.amazon.com/es/blogs/compute/new-aws-lambda-scaling-controls-for-kinesis-and-dynamodb-event-sources/\n\n\nAdvanced DynamoDB stream filtering\nNPM build process of lambdas\nTSConfig\n"}}